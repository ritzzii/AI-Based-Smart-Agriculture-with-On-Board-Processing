# -*- coding: utf-8 -*-
"""LOP_weed_classification_FL_(1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OSoQwNfHMRkIfU6YF84CI_VwPMeOoogb
"""

import os
import numpy as np
import pandas as pd
import tensorflow as tf
import keras
import sklearn
import matplotlib.pyplot as plt
import cv2
import PIL

from glob import glob
from tqdm import tqdm
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import Model, Sequential
from tensorflow.keras.applications import EfficientNetB0,ResNet50
from tensorflow.keras.layers import Dropout, Dense, Flatten, ConvLSTM2D,Conv2D,MaxPool2D,GlobalMaxPool2D,GlobalAveragePooling2D,BatchNormalization,Input,Reshape,GlobalAveragePooling2D
from sklearn.model_selection import StratifiedKFold,train_test_split
from sklearn.metrics import classification_report,accuracy_score,precision_score,recall_score,f1_score,roc_auc_score,roc_curve,confusion_matrix
from keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.utils import plot_model
from tensorflow.keras.layers.experimental import preprocessing
#from tensorflow.keras.applications.efficientnet import preprocess_input
from tensorflow.keras.applications.resnet import preprocess_input
from mlxtend.plotting import plot_confusion_matrix
from sklearn.utils import shuffle
from tabulate import tabulate
from sklearn.preprocessing import OneHotEncoder
from tensorflow.keras import backend as K
from PIL import Image

os.chdir('/content/drive/MyDrive/LOP_AI_BASED/Deepweed')
!unzip '/content/drive/MyDrive/LOP_AI_BASED/deepweed.zip'

df=pd.read_csv('/content/drive/MyDrive/LOP_AI_BASED/modified_labels.csv')

df=df.drop(['Label'], axis=1)
df.head()

"""**Checking the number of NULL datapoints :**"""

print("Tain Data Shape:",df.shape)
df.isna().sum()

""" **Observing the various categories :**"""

print("Total categories: ",len(df['Species'].unique()))
print()
print("Categories: ",df['Species'].unique())
df['Species'].value_counts().plot(kind='bar')
df['Species'].value_counts()

"""**Preprocessing Deepweed data set :**"""

data_dir = "/content/drive/MyDrive/LOP_AI_BASED/Deepweed/"
image_h,image_w=112,112
classes = df['Species'].unique()
image_names=df['Filename']
target_label=df['Species']

image_list=[]
for img in tqdm(image_names):
  image_list.append(np.asarray(preprocess_input((Image.open(data_dir+img)).resize((image_h,image_w))).astype('uint8')))

np.save('/content/drive/MyDrive/LOP_AI_BASED/res_p_resized_train_112.npy',image_list)

image_list=np.load('/content/drive/MyDrive/LOP_AI_BASED/res_p_resized_train_112.npy')

image_list.shape

image_list=np.asarray(image_list)/255.0

plt.figure(figsize=(10,10))
for i in range(9):
  plt.subplot(330 + 1 + i)
  plt.imshow(image_list[i])

y=np.asarray(target_label).reshape(-1,1)

"""**Encode categorical features as a one-hot numeric array :**"""

enc = OneHotEncoder()
y=enc.fit_transform(y).toarray()

"""**Creating train - test split :**
15% data for testing the global trained model 
"""

X_train, X_test, y_train, y_test = train_test_split(image_list,y,test_size=0.15, random_state=42)

"""**Creating Federated members (clients) :**
Each client will have its own data coupled to it. To serve the purpose, the whole dataset is divided into equal shards according to the number of clients. 
"""

def create_clients(image_list, y, num_clients, initial='clients'):
    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]
    data = list(zip(image_list, y))
    shuffle(data)
    size = len(data)//num_clients
    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]
    assert(len(shards) == len(client_names))
    return {client_names[i] : shards[i] for i in range(len(client_names))}

clients = create_clients(X_train, y_train, num_clients=3, initial='client')

"""**Processing and batching the data :**
 Data per client is divided into smaller batches
"""

def batch_data(data_shard, bs=16):

    data, label = zip(*data_shard)
    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))
    return dataset.shuffle(len(label)).batch(bs)

clients_batched = dict()
for (client_name, data) in clients.items():
    clients_batched[client_name] = batch_data(data)
    
test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))

"""```
{
  clients_1:{{d1.....d16},{d17...d32}...},
  clients_2:{{d1.....d16},{d17...d32}...},
  clients_3:{{d1.....d16},{d17...d32}...}
}
```

class EBN:
    @staticmethod
    def build(shape, classes):
        model = Sequential()
        model.add(EfficientNetB0(weights='imagenet',include_top=False,input_shape=(shape,shape,3)))
        model.add(GlobalAveragePooling2D())
        #model.add(Dropout(0.5))
        model.add(BatchNormalization())
        model.add(Dense(classes, activation='softmax'))
        return model

**Creating the ResNet50 model :**
"""

class RNT:
    @staticmethod
    def build(shape, classes):
      resn=ResNet50(weights='imagenet',include_top=False,input_shape=(shape,shape,3))
      for layer in resn.layers[-25:]:
        layer.trainable = False
      model = Sequential()
      model.add(resn)
      model.add(GlobalAveragePooling2D())
      #model.add(Dropout(0.2))
      model.add(BatchNormalization())
      model.add(Dense(classes, activation='softmax'))
      return model

mdl = RNT()
cmdl = mdl.build(image_h, 9)
cmdl.summary()

plot_model(cmdl,show_shapes=True)

"""**Setting hyperparameters and Optimizer:**"""

lr = 0.008
comms_round = 30
loss='categorical_crossentropy'
metrics = ['accuracy']
optimizer = Adam(learning_rate=lr,decay=lr/comms_round)

"""**Model Aggregation (Federated Averaging) :**

Calculates the proportion of a client’s local training data with the overall training data held by all clients
"""

def weight_scalling_factor(clients_trn_data, client_name):
    client_names = list(clients_trn_data.keys())
    bs = list(clients_trn_data[client_name])[0][0].shape[0]
    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs
    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs
    return local_count/global_count

"""Scales each of the local model’s weights based the value of their scaling factor calculated in weight_scalling_factor"""

def scale_model_weights(weight, scalar):
    weight_final = []
    steps = len(weight)
    for i in range(steps):
        weight_final.append(scalar * weight[i])
    return weight_final

"""Sums all clients’ scaled weights together"""

def sum_scaled_weights(scaled_weight_list):
    avg_grad = list()
    for grad_list_tuple in zip(*scaled_weight_list):
        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)
        avg_grad.append(layer_mean)
        
    return avg_grad

def test_model(X_test, Y_test,  model, comm_round):
    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)
    #logits = model.predict(X_test, batch_size=100)
    logits = model.predict(X_test)
    loss = cce(Y_test, logits)
    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))
    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))
    return acc, loss

acc_history=[]
loss_history=[]

"""**Federated Model Training**"""

fed_global = RNT()
global_model = fed_global.build(image_h, 9)
        

for comm_round in range(comms_round):
    global_weights = global_model.get_weights()
    scaled_local_weight_list = list()

    client_names= list(clients_batched.keys())
    shuffle(client_names)

    for client in client_names:
        fed_local = RNT()
        local_model = fed_local.build(image_h, 9)
        local_model.compile(loss=loss, 
                      optimizer=optimizer, 
                      metrics=metrics)
        
        local_model.set_weights(global_weights)
        local_model.fit(clients_batched[client], epochs=3, verbose=0)
        
        scaling_factor = weight_scalling_factor(clients_batched, client)
        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)
        scaled_local_weight_list.append(scaled_weights)
        
        K.clear_session()
        
    average_weights = sum_scaled_weights(scaled_local_weight_list)
    
    global_model.set_weights(average_weights)

    for(X_test, Y_test) in test_batched:
        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)

    acc_history.append(global_acc)
    loss_history.append(global_loss)

plt.figure(figsize=(12,9))
plt.plot(loss_history,'r')
plt.title('Global loss')
plt.ylabel('Loss')
plt.xlabel('Comm_rounds')
plt.show()

plt.figure(figsize=(12,9))
plt.plot(acc_history,'g')
plt.title('Global Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Comm_rounds')
plt.show()

y_pred=np.argmax(global_model.predict(X_test),axis=-1)

import seaborn as sns

cnf=confusion_matrix(np.argmax(Y_test,axis=-1),y_pred)
plt.figure(figsize=(12,9))
sns.heatmap(cnf,annot=True)

cnf

